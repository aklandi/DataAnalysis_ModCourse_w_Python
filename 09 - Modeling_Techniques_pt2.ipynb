{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Topic: Modeling Techniques Continued\n",
    "\n",
    "<ins>Learning Objectives</ins>\n",
    "\n",
    "1. To learn basic machine learning methods\n",
    "\n",
    "This is *NOT* a machine learning course.  We will only learn when to use each method, how to implement in Python, and how to analyze the output.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section IV: Clustering with K-Means**\n",
    "\n",
    "K-Means is a machine learning method for clustering.\n",
    "\n",
    "* Clustering is the act of grouping data based on similarities in certain **quantitative** attributes.  For example, perhaps you tell Tinder that you only want to see people who are at least 6 feet tall.  Tinder will obviously recommend people greater than 6 feet tall, but it might also recommend people who are between 5'9\" and 6'.\n",
    "* K-Means is a method (there are many others!) that creates $k$ groups based on distance between data points.  I will go over this method here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall how to load the iris data set from Notebook 05\n",
    "import ___ as sbn\n",
    "iris_DF = __\n",
    "\n",
    "# in this line, go ahead and check out the column names and the first few rows like we did in Notebook 05!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the iris data set has four quantitative variables for each iris.  A *quantitative* variable is data that has a unit of measurement that is either continuous, like inches, or discrete, like number of cars someone owns.\n",
    "\n",
    "Perhaps we want to cluster irises together based on \"sepal length\" and \"petal length\".  Let's look at a graph to see if this makes sense to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_DF.plot(kind = 'scatter', x = 'sepal_length', y = 'petal_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Are there any clear groupings based on the graph?  If so, how many do you think there are?  Use this textbox to answer.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means creates \"circles\" around groups in the data.  \n",
    "\n",
    "* Initialize $k$ means, called $\\bar{x}_i$, for i = 1 to k.\n",
    "* For all points, if they are less than $r$ distance away from a particular $\\bar{x}_i$, then they are assigned to the $i$-th group.\n",
    "* The $k$ means are re-calculated based on the data points in their groups.\n",
    "* Start this process over again.  Do this $t$ times.\n",
    "\n",
    "We, as the data analysts, select \n",
    "\n",
    "* $k$ (the number of clusters to create), and \n",
    "* $t$ (the number of times to repeat the process).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Explain the process above in your own words.  This will help you synthesize the information.  If it helps, use an example or try to explain to a classmate or draw a picture!  Insert any of this work here.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use sklearn, the most common machine learning library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_clusters = 2\n",
    "t_times = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_clusters, max_iter = t_times)\n",
    "predicted_labels = kmeans.fit_predict(X = iris_DF.loc[:,['sepal_length','petal_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will give us the cluster number for each data point.  Let's check this out to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x = iris_DF.loc[:,'sepal_length'], y = iris_DF.loc[:,'petal_length'], c = predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Check out how the clusters change when you change the number of clusters and the number of iterations.  \n",
    "Try it out in new code chunks and text boxes below.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section V: Classification with K-Nearest Neighbors**\n",
    "\n",
    "K-Nearest Neighbors is a machine learning method for classification.\n",
    "\n",
    "* Classification is the act of grouping data based on some **qualitative** attribute.  For example, once you've watched *Mean Girls* on Netflix, it might recommend *Freaky Friday* to you next because \\<lindsay lohan\\>, \\<teen\\>, \\<movie\\>, \\<romcom\\> are all attributes of *Mean Girls* shared with *Freaky Friday*.\n",
    "* K-Nearest Neighbors is a method that determines which group a data point belongs to based on the most common attributes of the $k$ nearest neighbors.  I will go over this method here.\n",
    "\n",
    "We'll stick with the iris dataset!  We first split the data into two subsets, a smaller data set of irises that we'll try to classify correctly based on a larger subset of irises.  That is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training data \n",
    "* test data\n",
    "\n",
    "This is quite common to do!  Usually you want about 68-85% of the original as your training data and the remainder as the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sticking with sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setting up the data for the function\n",
    "X = iris_DF[['sepal_length', 'petal_length']]\n",
    "y = iris_DF['species']\n",
    "\n",
    "# split the data so that 82% is in the training set\n",
    "train_X, test_X, train_y, test_y = train_test_split(X , y, test_size = 0.12)\n",
    "(n_train, m_train) = train_X.shape\n",
    "n_test = len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors classifies new data.\n",
    "\n",
    "* Determines the $k$ nearest neighbors to a new data point.\n",
    "* Checks out how those neighbors are classified.\n",
    "* Gives the new data point the classification most common for those $k$ neighbors.\n",
    "\n",
    "As the data analysts, we only specify \n",
    "\n",
    "* $k$, the number of neighbors we should care about.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sticking with sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_neighbors = 3\n",
    "# train the model\n",
    "knn_model = KNeighborsClassifier(k_neighbors).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Explain the process above in your own words.  This will help you synthesize the information.  If it helps, use an example or try to explain to a classmate or draw a picture!  Insert any of this work here. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accurate the predicted iris class for the data in *test_X* is by comparing it to the truth! called *test_y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_model.predict(test_X)\n",
    "score = np.sum(y_pred == test_y)/n_test\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also graph our results for K-Nearest Neighbors.  <span style=\"color:purple\">Describe a visualization that would make the most sense for what KNN does.</span>  You do not need to create one, but bonus points if you do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section VI: Prediction with Linear Regression**\n",
    "\n",
    "Linear regression can be used to make predictions!  At the start, linear regression helped us to answer \n",
    "\n",
    "1. Is there really a linear relationship between the explanatory and response variables in the population (all objects) or might the pattern we see in the scatterplot plausibly arise just by chance?\n",
    "\n",
    "2. What is the rate of change that relates the response variable to the explanatory variable in the population (all objects), including the margin of error for our estimate of the slope?\n",
    "\n",
    "Here, we will answer different questions\n",
    "\n",
    "3. Given a new data point $x = a$, what is the $y$-value predicted by the line?\n",
    "4. What is the margin of error for this prediction?  That is, under a slightly different model, how much will the prediction change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll revisit the motivating problem from notebook 08.\n",
    "\n",
    "<ins>Motivating Problem</ins> \"The bigger they are, the harder they fall.\"  \n",
    "Below is the weight (kg) of 5 different objects and the force (kg $\\cdot$ m/s $^2$) with which they hit the ground (taking into consideration there is air resistance).\n",
    "\n",
    "| weight | force |\n",
    "| ------ | ----- |\n",
    "| 45.3 | 443.94 |\n",
    "| 22.6 | 221.48 |\n",
    "| 34.5 | 338.10 |\n",
    "| 0.91 | 8.82 |\n",
    "| 38.6 | 378.29 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the original data at the start of this very long notebook, *DF*.  And we'll be working with the same least squares model as earlier, ```lsq```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.regression.linear_model as lm\n",
    "import seaborn as sbn\n",
    "\n",
    "# create a pandas data frame of our data\n",
    "DF = pd.DataFrame(data = np.array([[45.3, 22.6,34.5, 0.91, 38.6], [443.94, 221.48, 338.10, 8.82, 378.29]]).T, columns = [\"weight\", \"force\"])\n",
    "lsq = lm.OLS(endog = DF['force'], exog = DF['weight']).fit()\n",
    "\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```lsq``` to predict the force with which an object weighing 55.1 kg hits the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsq.get_prediction(55.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Why doesn't this work?  Check out ```help(lsq.predict)```.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lsq.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(np.array([55.1]), columns = ['weight'], index = [0])\n",
    "predict_y = lsq.get_prediction(new_data)\n",
    "\n",
    "predict_y.predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">Does the result to seem to accurately fit the relationship between weight and force?  Why or why not? Answer in this textbox. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y.summary_frame(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code chunk above produces an output: mean, mean_se, mean_ci_lower, mean_ci_lower, obs_ci_lower, obs_ci_upper\n",
    "\n",
    "* get_prediction is just the result of plugging $x=55.1$ into $\\hat{y} = m\\cdot x + b$, where slope $m$ and intercept $b$ is found by the least squares method OLS. \n",
    "* but we need to know how much variation there would be in the prediction, so we need to use summary_frame and look at the obs_ci_lower and obs_ci_upper because this tells us that if we randomly sampled another object that weighed 55.1 kg, it will result in a force between 539.81 and 540.15, 95% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Quiz!\n",
    "\n",
    "<span style=\"color:purple\">Write as much as possible here</span>.  Your answers provide you guidance later during your projects.\n",
    "\n",
    "1.   When do we want to cluster data?  What does it mean to cluster data?\n",
    "2.   When do we want to classify data?  What does it mean to classify data?\n",
    "3.   Why might we want to use the regression line to make predictions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
